# OnlineAnySeg: Online Zero-Shot 3D Segmentation by Visual Foundation Model Guided 2D Mask Merging
### [Project Page](https://yjtang249.github.io/OnlineAnySeg/) | [Paper](https://arxiv.org/pdf/2503.01309)

###### [Yijie Tang*](https://github.com/yjtang249), [Jiazhao Zhang*](https://jzhzhang.github.io/), Yuqing Lan, [Yulan Guo](https://www.yulanguo.cn), Dezun Dong, [Chenyang Zhu†](https://www.zhuchenyang.net), [Kai Xu†](https://kevinkaixu.net)

\*Equal Contribution, †Corresponding Authors

> Online 3D instance segmentation of a progressively reconstructed scene is both a critical and challenging task for embodied applications. With the success of visual foundation models (VFMs) in the image domain, leveraging 2D priors to address 3D online segmentation has become a prominent research focus. Since segmentation results provided by 2D priors often require spatial consistency to be lifted into final 3D segmentation, an efficient method for identifying spatial overlap among 2D masks is essential—yet existing methods rarely achieve this in real time, mainly limiting its use to offline approaches. To address this, we propose an efficient method that lifts 2D masks generated by VFMs into a unified 3D instance using a hashing technique. By employing voxel hashing for efficient 3D scene querying, our approach reduces the time complexity of costly spatial overlap queries from $O(n^2)$ to $O(n)$. Accurate spatial associations further enable 3D merging of 2D masks through simple similarity-based filtering in a zero-shot manner, making our approach more robust to incomplete and noisy data. Evaluated on the ScanNet and SceneNN benchmarks, our approach achieves state-of-the-art performance in online, 3D instance segmentation with leading efficiency.

<!-- ![image](figs/teaser.png) -->
<div align=center>
<img src=figs/teaser1.png width=50%/>
</div>


## TODOs

* [X] Release the main code of OnlineAnySeg.
* [ ] Release the evaluation code.
* [ ] Improvement: replace the threshold-based mask merging strategy with an adaptive approach.


## 1. Data preparation
For dataset preparation, please refer to the following documentation, which introduce how to prepare data from [ScanNet](https://kaldir.vc.in.tum.de/scannet_benchmark/), [SceneNN](https://hkust-vgd.github.io/scenenn/) and customized sequence.

* [Data preparation](./docs/data_prep.md)


## 2. Installation
(Step 1) Create conda environment, then install PyTorch and other dependencies.
```
conda create -n OASeg python=3.9
conda activate OASeg
pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 -f https://download.pytorch.org/whl/torch_stable.html
pip install -r requirements.txt
```

(Step 2) Install [PyTorch3D](https://github.com/facebookresearch/pytorch3d)
```
pip install "git+https://github.com/facebookresearch/pytorch3d.git"
```
or install from a local clone
```
git clone https://github.com/facebookresearch/pytorch3d.git
cd pytorch3d && pip install -e .
```

(Step 3) CropFormer

The official installation of Cropformer is composed of two steps: installing detectron2 and then Cropformer. Following  the installation steps in MaskClustering, the two steps are combined into the following scripts. If you have any problems, please refer to the installation guide in [MaskClustering](https://github.com/PKU-EPIC/MaskClustering) or the original [Cropformer](https://github.com/qqlu/Entity/blob/main/Entityv2/CropFormer/INSTALL.md) installation guide.
```bash
cd third_party
git clone git@github.com:facebookresearch/detectron2.git
cd detectron2
pip install -e .

cd ../
git clone git@github.com:qqlu/Entity.git
cp -r Entity/Entityv2/CropFormer detectron2/projects
cd detectron2/projects/CropFormer/entity_api/PythonAPI
make

cd ../..
cd mask2former/modeling/pixel_decoder/ops
sh make.sh
pip install -U openmim
mim install mmcv
```
We add additional scripts into cropformer to make it sequentialy process all sequences.
```bash
cd ../../../../../../../../
cp scripts/mask_predict/* third_party/detectron2/projects/CropFormer/demo_cropformer
```
Finally, download the [CropFormer checkpoint](https://huggingface.co/datasets/qqlu1992/Adobe_EntitySeg/blob/main/CropFormer_model/Entity_Segmentation/Mask2Former_hornet_3x/Mask2Former_hornet_3x_576d0b.pth) and [CLIP checkpoint](https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K/blob/main/open_clip_pytorch_model.bin) and place them in the `./models` directory at the root of the project.


(Step 4) FCGF

In our method, we use FCGF to extract geometric features from 3D point cloud. The [FCGF](https://github.com/chrischoy/FCGF) repository is already placed at 'third_party/'. If you have any problems, please follow the [FCGF](https://github.com/chrischoy/FCGF) installation guide.

Install MinkowskiEngine:
```
pip install git+https://github.com/NVIDIA/MinkowskiEngine.git
```
If it is not installed successfully, you can try to install it from a local clone:
```
cd third_party
git clone https://github.com/NVIDIA/MinkowskiEngine.git
cd MinkowskiEngine && python setup.py install --blas_include_dirs=${CONDA_PREFIX}/include --blas=openblas
```


## 3. Running
### Run a given sequence (demo)
Here we introduce how to run a given sequence (ScanNet, SceneNN or customized sequence).

(Step 1) To segment a given sequence, and save detected 2D masks as well as corresponding CLIP features to the specified dir, you can run the following command:
```
# python third_party/detectron2/projects/CropFormer/demo_cropformer/mask_predict_single_seq_w_semantic.py \
    --root <root_dir_of_dataset> \
    --seq_name <scene_id> \
    --output_root <seg_output_dir_of_dataset> \
    --pretrained_path <CLIP_checkpoint_path> \
    --opts MODEL.WEIGHTS <Cropformer_checkpoint_path>
```
For example, if the given sequence is "scene0011_00" in [ScanNet](https://kaldir.vc.in.tum.de/scannet_benchmark/), the command to run is:
```
# python third_party/detectron2/projects/CropFormer/demo_cropformer/mask_predict_single_seq_w_semantic.py \
    --root ./data/scannet \
    --seq_name scene0011_00 \
    --output_root ./data/scannet/seg_result \
    --pretrained_path ./models/open_clip_pytorch_model.bin \
    --opts MODEL.WEIGHTS ./models/Mask2Former_hornet_3x_576d0b.pth
```
After running the command, you can find the following directory structure in `<seg_output_dir_of_dataset>/<scene_id>`:
```
<seg_output_dir_of_dataset>
    ├── <scene_id>
         ├── mask               # segmented 2D masks of each frame
         ├── mask_color         # segmented 2D masks of each frame (only for visualization)
         └── mask_embeddings    # extracted CLIP features of the 2D masks
```
For other datasets or customized sequences, you can adjust the command-line arguments accordingly. As long as the segmentation results follow the directory structure shown above, the subsequent steps should work seamlessly.

(Step 2) process the RGB-D sequence:
```
# python main.py -c <config_file> --seq_name <scene_id> -d <sequence_dir> -i <seg_sequence_dir>
```
For example, if the given sequence is "scene0011_00" in [ScanNet](https://kaldir.vc.in.tum.de/scannet_benchmark/), the command to run is:
```
# python main.py -c config/scannet_cropformer.yaml --seq_name scene0011_00 -d ./data/scannet/scene0011_00/frames \
    -i ./data/scannet/seg_result/scene0011_00
```
When finished, the output will be found in the `./output/` directory by default.


### Run all scenes in the dataset
TODO


## Acknowledgement
Parts of the code are modified from [MaskClustering](https://github.com/PKU-EPIC/MaskClustering) and [OpenFusion](https://github.com/UARK-AICV/OpenFusion). Thanks to the original authors.


## Citation
If you find our code or paper useful, please cite
```
@article{tang2025onlineanyseg,
  title={OnlineAnySeg: Online Zero-Shot 3D Segmentation by Visual Foundation Model Guided 2D Mask Merging},
  author={Tang, Yijie and Zhang, Jiazhao and Lan, Yuqing and Guo, Yulan and Dong, Dezun and Zhu, Chenyang and Xu, Kai},
  journal={arXiv preprint arXiv:2503.01309},
  year={2025}
}
```
